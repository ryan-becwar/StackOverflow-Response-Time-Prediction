{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting response times for StackOverflow questions\n",
    "Aaron Pereira and Ryan Becwar</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The main aim of this study is to try to analyze the MSR2019 SOTorrent dataset to try to gain some insight about stackoverflow data and try to predict the amount of time a question would take to find an accepted answer. We tried to make this prediction based on various factors based on the properties of the user asking the question and the question itself. We tried to list down some features based on these properties that we could identify and tried feeding them into different machine learning models and tried to find some correlation between these features and the response time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial problems related to dataset usage\n",
    "\n",
    "- The MSR-2019 page linked to BigQuery but the link seemed to be dead\n",
    "- We found an alernate BigQuery link but we had to make a google account to access that\n",
    "- Free/Trial accounts had a 1TB processing limit and our queries processed more data than that\n",
    "\n",
    "We then decided to go for the alternate link on the 'empirical-software.engineering' to access the dataset on Zenodo.com. We faced the following problems with this source\n",
    "\n",
    "- Zenodo hosted individual files for all the tables. We couldn't run interactive queries like BigQuery\n",
    "- Majority of our processing involved the 'Posts' table that came up to 70 GB once extracted\n",
    "- The data was available in XML format which needed additional processing overhead\n",
    "- the 'Posts' table also involved a join on itself which would be computationally challenging given its size\n",
    "- We also needed to process a few more tables to get all the data we needed\n",
    "\n",
    "We used a variety of pre-processing steps to get the data in a format that we deemed fit to run analysis on that will be explained in detail in the following setions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the dataset\n",
    "\n",
    "The dataset we planned to use was structured in the form of XML files. Each table joined others using foreign keys that were a little complicated to understand sometimes. The following is a pictorial representation of the dataset:\n",
    "\n",
    "<img src=\"https://empirical-software.engineering/assets/images/sotorrent_2018-12-09_model.png\" style=\"text-align:center\"></img>\n",
    "<center>source : https://empirical-software.engineering</center>\n",
    "    \n",
    "We wanted to center our research around 'Tags' and how they affect the ability of a question to recieve an accepted answer. We decided to use the following Tables for our research:\n",
    "\n",
    "- Posts\n",
    "- Tags\n",
    "- Users\n",
    "- Postlinks\n",
    "\n",
    "reputation,Votes,Downvotes,LinkType,TimeToAnswer,Question Body Char Count,Question Body Word Count,Question Title Word Count,Question Title Word Count,Question creation date,Title end in ?,Question creation datee\n",
    "\n",
    "We planned to extract the following fields from the tables mentioned above\n",
    "\n",
    "### Posts\n",
    "- Question character count\n",
    "- Question word count\n",
    "- Does the question end with a '?'\n",
    "- __Time taken to get an accepted answer (Question creation date - Accepted answer creation date)__\n",
    "\n",
    "### Users\n",
    "- Reputation\n",
    "- Total upvotes\n",
    "- Total downvotes\n",
    "- Number of badges the user has\n",
    "\n",
    "### Postlinks\n",
    "- type of links the questions have\n",
    "\n",
    "### Tags\n",
    "Tags seemed like the most important criteria and there were around 60000 unique tags present in the dataset. We needed to find an appropriate method to represent these tags in the predictions. For that, we followed the following procedure\n",
    "\n",
    "- Filter questions based on a programming language (extract only those questions that had 'C++' in the question, body or tags)\n",
    "- Extract all the tags from these questions\n",
    "- Rank the Tags based on the frequency of occurance\n",
    "- Shortlist the top 50 Tags\n",
    "- Have each tag represented as a binary feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "We first downloaded each data table from Zenodo. These tables were in the form of 7z archive files. We then extracted each one of these tables. They totalled to 110 GB of data on the whole.\n",
    "\n",
    "## Processing large XML files\n",
    "Consider 'Posts.xml'. This was a single XML file. No library would be able to process such a large XML object. So we split this file into smaller files using the following linux command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split -l50000 Comments.xml --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command splits the big file into smaller files having 50000 lines each. But this would now not be valid XML. We used the following script to convert all these split files into valid XML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "for filename in /media/aaron/1b564795-7979-4d3d-8f05-635dda73c1e4/cs515/*; do\n",
    "\tsed -i '1i<?xml version=\"1.0\" encoding=\"utf-8\"?><posts>' ${filename};\n",
    "\techo \"</posts>\" >> $filename;\n",
    "\t#sed -i '1d' $filename\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now converted the multiple split files into valid XML. We then ran the following python code snippet to get a CSV file of only the required features from each XML. Its important to note that we now divided the 'Posts' data into 'questions' and 'answers' based on 'PostType'. We also restricted the questions based on only those that contained 'C++'. The following is the code snippet we used for generating a CSV for 'Questions'. We used similar snippits to generate other CSVs too.\n",
    "\n",
    "There are a set of Lists created at the start. These Lists contain all the features that we needed to extract for all the tables. We ran a similar snippet for all the tables that we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import os\n",
    "commentsList = ['Id','CreationDate']\n",
    "userlist = ['UserId','Class','CreationDate','LastAccessDate','Views','UpVotes','DownVotes','AccountId']\n",
    "badgelist = ['UserId','Class']\n",
    "tagslist = ['Id','TagName']\n",
    "postLinklist = ['Id','PostId','LinkTypeId']\n",
    "questionslist = ['Id','AcceptedAnswerId','CreationDate','OwnerUserId', 'Tags', 'Title', 'Body']\n",
    "answerlist = ['Id','ParentId','CreationDate','OwnerUserId']\n",
    "extra = ['Title','Body']\n",
    "table = \"questions\"\n",
    "\n",
    "#filebasepath = \"/home/aaron/CS-515Project/\"\n",
    "filebasepath = \"/media/aaron/1b564795-7979-4d3d-8f05-635dda73c1e4/\"\n",
    "\n",
    "with open(table+'.csv', 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    count = 0\n",
    "    for filename in os.listdir(filebasepath+table+\"/\"):\n",
    "        try:\n",
    "            tree = ET.parse(filebasepath+table+'/'+filename)\n",
    "            root = tree.getroot()\n",
    "            for elem in root:\n",
    "                writeContent = []\n",
    "                postType = int(elem.attrib['PostTypeId'])\n",
    "                if postType == 1 and \"c++\" in elem.attrib['Tags']:\n",
    "                    for attrib in elem.attrib:\n",
    "                        if attrib in questionslist:\n",
    "                            if attrib not in extra:\n",
    "                                writeContent.append(elem.attrib[attrib]);\n",
    "                            else:\n",
    "                                if attrib == \"Title\":\n",
    "                                    if str(elem.attrib[attrib]).endswith(\"?\"):\n",
    "                                        writeContent.append(0)\n",
    "                                        writeContent.append(elem.attrib[attrib].replace(\",\",\"_\"))\n",
    "                                        writeContent.append(len(elem.attrib[attrib]))\n",
    "                                        writeContent.append(len(elem.attrib[attrib].split(\" \")))\n",
    "                                    else:\n",
    "                                        writeContent.append(1);\n",
    "                                        writeContent.append(elem.attrib[attrib].replace(\",\",\"_\"))\n",
    "                                        writeContent.append(len(elem.attrib[attrib]))\n",
    "                                        writeContent.append(len(elem.attrib[attrib].split(\" \")))\n",
    "                                if attrib == \"Body\":\n",
    "                                    writeContent.append(len(elem.attrib[attrib]))\n",
    "                                    writeContent.append(len(elem.attrib[attrib].split(\" \")))\n",
    "\n",
    "\n",
    "                    if len(writeContent)==len(questionslist)+4:\n",
    "                        writer.writerow(writeContent)\n",
    "        except:\n",
    "            print(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting data into MySql\n",
    "\n",
    "The code above generated a bunch of CSV files for us. We now decided that it would be best to place these files into a MySql database and perform a join to get the data we needed. We used the following command to load this data into our MySql database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD DATA INFILE '/var/lib/mysql-files/questions.csv' \n",
    "INTO TABLE questions \n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "IGNORE 0 ROWS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command was executed for every CSV file and we made a corresponding table for each CSV file. Our total data was now reduced to 1.3 GB. We then ran the following query to extract the final dataset and saved the results as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select Tags, users.Reputation as 'reputation',\n",
    "\t\t\t\tusers.Upvotes as 'Votes', \n",
    "\t\t\t\tusers.DownVotes as 'Downvotes', \n",
    "                postlinks.LinkTypeId as 'LinkType',  \n",
    "                TIMESTAMPDIFF(MINUTE,questions.CreationDate,answers.CreationDate) as 'TimeToAnswer',\n",
    "                questions.BodyCharCount as 'Question Body Char Count',\n",
    "                questions.BodyWordCount as 'Question Body Word Count',\n",
    "                questions.TitleCharCount as 'Question Title Char Count',\n",
    "                questions.TitleWordCount as 'Question Title Word Count',\n",
    "                questions.CreationDate as 'Question creation date', \n",
    "                questions.TitleQuestionMark as 'Title end in ?',\n",
    "                answers.CreationDate as 'Question creation date' \n",
    "                from questions \n",
    "\t\t\t\t\t\tleft join answers on questions.AcceptedAnswerId = answers.Id \n",
    "\t\t\t\t\t\tleft join users on questions.OwnerUserId = users.Id \n",
    "                        left join postlinks on questions.Id = PostId;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Tag Features\n",
    "\n",
    "The last step was to process extract the top 50 tags and represent them as one feature column each. First we needed to extract the 50 most common tags. We extracted the list of all tags using the following query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select tags from questions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now cleaned the data a little as each tag was enclosed in '< >'. We cleaned these tags and placed them in an online word frequency generator. the most common tags that were observed were he following:\n",
    "\n",
    "\"c++\",\"c++11\",\"qt\",\"templates\",\"visual-c++\",\"boost\",\"arrays\",\"pointers\",\"vector\",\"windows\",\"winapi\",\"stl\",\"multithreading\",\"string\",\"opencv\",\"linux\",\"class\",\"opengl\",\"gcc\",\"visual-studio\",\"algorithm\",\"inheritance\",\"visual-studio-2010\",\"function\",\"mfc\",\"java\",\"c++14\",\"g++\",\"dll\",\"python\",\"operator-overloading\",\"struct\",\"c++-cli\",\"oop\t3307\",\"memory-management\",\"iterator\",\"performance\",\"cmake\",\"memory\",\"linker\",\"sockets\",\"reference\",\"compiler-errors\",\"language-lawyer\",\"macos\",\"const\",\"std\",\"lambda\",\"file\",\"debugging\"\n",
    "\n",
    "We now used the following the following code snippet to generate tag based features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "tagList = [\"c++\",\"c++11\",\"qt\",\"templates\",\"visual-c++\",\"boost\",\"arrays\",\"pointers\",\"vector\",\"windows\",\"winapi\",\"stl\",\"multithreading\",\"string\",\"opencv\",\"linux\",\"class\",\"opengl\",\"gcc\",\"visual-studio\",\"algorithm\",\"inheritance\",\"visual-studio-2010\",\"function\",\"mfc\",\"java\",\"c++14\",\"g++\",\"dll\",\"python\",\"operator-overloading\",\"struct\",\"c++-cli\",\"oop\t3307\",\"memory-management\",\"iterator\",\"performance\",\"cmake\",\"memory\",\"linker\",\"sockets\",\"reference\",\"compiler-errors\",\"language-lawyer\",\"macos\",\"const\",\"std\",\"lambda\",\"file\",\"debugging\"]\n",
    "\n",
    "frame = pd.read_csv('newFull.csv')\n",
    "\n",
    "with open(\"encodingFull.csv\", 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    first = True\n",
    "    with open('newFull.csv') as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            if first:\n",
    "                writer.writerow(tagList + row[1:len(row)])\n",
    "                first = False\n",
    "            else:\n",
    "                rowsAfterTags = row[1:len(row)]\n",
    "                tags = row[0][1:-1].split(\"><\")\n",
    "                tagFeatures = []\n",
    "                for tag in tagList:\n",
    "                    if tag in tags:\n",
    "                        tagFeatures.append(1)\n",
    "                    else:\n",
    "                        tagFeatures.append(0)\n",
    "                totalFeatureRow = tagFeatures + rowsAfterTags\n",
    "                writer.writerow(totalFeatureRow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finally generated the final dataset in the form of a CSV file that we can then use to perform data analysis. Our final dataset has a total of 367k entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
